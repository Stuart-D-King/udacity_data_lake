{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk-pom:1.10.34\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = 's3a://udacity-dend/'\n",
    "output_data = 's3://sking-data-engineer/data-lake/parquet-files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "# song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "song_data = input_data + 'song_data/A/A/*/*.json'\n",
    "\n",
    "songSchema = t.StructType([\n",
    "    t.StructField('artist_id', t.StringType()),\n",
    "    t.StructField('artist_latitude', t.DoubleType()),\n",
    "    t.StructField('artist_location', t.StringType()),\n",
    "    t.StructField('artist_longitude', t.DoubleType()),\n",
    "    t.StructField('artist_name', t.StringType()),\n",
    "    t.StructField('duration', t.DoubleType()),\n",
    "    t.StructField('num_songs', t.IntegerType()),\n",
    "    t.StructField('song_id', t.StringType()),\n",
    "    t.StructField('title', t.StringType()),\n",
    "    t.StructField('year', t.IntegerType()),\n",
    "])\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(song_data, schema=songSchema)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').dropDuplicates()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "# songs_output = output_data + 'songs'\n",
    "# songs_table.write.partitionBy('year', 'artist_id').parquet(songs_output, mode='overwrite') # 'error' is default\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.selectExpr('artist_id', 'artist_name as name', 'artist_location as location', 'artist_latitude as latitude', 'artist_longitude as longitude').dropDuplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "# artists_output = output_data + 'artists'\n",
    "# artists_table.write.parquet(artists_output, mode='overwrite') # 'error' is default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "# log_data = input_data + 'log_data/2018/11/*.json'\n",
    "log_data = [input_data + 'log_data/2018/11/2018-11-01-events.json', \\\n",
    "            input_data + 'log_data/2018/11/2018-11-02-events.json']\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table\n",
    "users_table = df.selectExpr('userId as user_id', 'firstName as first_name', 'lastName as last_name', 'gender', 'level').dropDuplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "# users_output = output_data + 'users'\n",
    "# users_table.write.parquet(users_output, mode='overwrite')\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = f.udf(lambda x: datetime.fromtimestamp(x/1000), t.TimestampType())\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = f.udf(lambda x: datetime.fromtimestamp(x/1000), t.DateType())\n",
    "\n",
    "df = df.withColumn('start_time', get_timestamp(f.col('ts')))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select('start_time', \\\n",
    "        f.hour('start_time').alias('hour'), \\\n",
    "        f.dayofmonth('start_time').alias('day'), \\\n",
    "        f.weekofyear('start_time').alias('week'), \\\n",
    "        f.month('start_time').alias('month'), \\\n",
    "        f.year('start_time').alias('year'), \\\n",
    "        f.date_format('start_time', 'u').cast(t.IntegerType()).alias('weekday')).dropDuplicates()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "# time_output = output_data + 'time'\n",
    "# time_table.write.partitionBy('year', 'month').parquet(time_output, mode='overwrite')\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "# songs_data = output_data + 'songs/*/*/*.parquet'\n",
    "# songs_df = spark.read.parquet(songs_data)\n",
    "\n",
    "# song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "song_data = input_data + 'song_data/A/A/*/*.json'\n",
    "\n",
    "songSchema = t.StructType([\n",
    "    t.StructField('artist_id', t.StringType()),\n",
    "    t.StructField('artist_latitude', t.DoubleType()),\n",
    "    t.StructField('artist_location', t.StringType()),\n",
    "    t.StructField('artist_longitude', t.DoubleType()),\n",
    "    t.StructField('artist_name', t.StringType()),\n",
    "    t.StructField('duration', t.DoubleType()),\n",
    "    t.StructField('num_songs', t.IntegerType()),\n",
    "    t.StructField('song_id', t.StringType()),\n",
    "    t.StructField('title', t.StringType()),\n",
    "    t.StructField('year', t.IntegerType())\n",
    "])\n",
    "\n",
    "# read song data file\n",
    "songs_df = spark.read.json(song_data, schema=songSchema)\n",
    "\n",
    "songplays_table = df.join(songs_df, on=[df.artist == songs_df.artist_name, df.song == songs_df.title]) \\\n",
    "    .select(df.start_time,\n",
    "        f.year(df.start_time).alias('year'),\n",
    "        f.month(df.start_time).alias('month'),\n",
    "        df.userId.alias('user_id').cast(t.IntegerType()),\n",
    "        df.level,\n",
    "        songs_df.song_id,\n",
    "        songs_df.artist_id,\n",
    "        df.sessionId.alias('session_id'),\n",
    "        df.location,\n",
    "        df.userAgent.alias('user_agent')) \\\n",
    "    .withColumn('songplay_id', f.monotonically_increasing_id())\n",
    "\n",
    "# df.createOrReplaceTempView('log_table')\n",
    "# songs_df.createOrReplaceTempView('songs_table')\n",
    "\n",
    "# # extract columns from joined song and log datasets to create songplays table\n",
    "# songplays_table = spark.sql('''\n",
    "#     select\n",
    "#     l.start_time,\n",
    "#     year(l.start_time) as year,\n",
    "#     month(l.start_time) as month,\n",
    "#     cast(l.userId as int) as user_id,\n",
    "#     l.level,\n",
    "#     s.song_id,\n",
    "#     s.artist_id,\n",
    "#     l.sessionId as session_id,\n",
    "#     l.location,\n",
    "#     l.userAgent as user_agent\n",
    "#     from log_table l\n",
    "#     join songs_table s on s.artist_name = l.artist and s.title = l.song\n",
    "# ''')\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "# songplays_output = output_data + 'songplays'\n",
    "# songplays_table.write.partitionBy('year', 'month').parquet(songplays_output, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.groupBy('level') \\\n",
    "    .agg(f.count('songplay_id').alias('song_plays')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.groupBy('gender') \\\n",
    "    .agg(f.countDistinct('user_id').alias('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.join(time_table, on=[songplays_table.start_time == time_table.start_time]) \\\n",
    "    .groupBy('hour') \\\n",
    "    .agg(f.countDistinct('songplay_id').alias('song_plays')) \\\n",
    "    .orderBy('hour') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=config.get('AWS','AWS_ACCESS_KEY_ID'),\\\n",
    "                    aws_secret_access_key=config.get('AWS','AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udacityBucket =  s3.Bucket('udacity-dend')\n",
    "\n",
    "for obj in udacityBucket.objects.filter(Prefix='log_data'):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udacityBucket =  s3.Bucket('udacity-dend')\n",
    "\n",
    "for obj in udacityBucket.objects.filter(Prefix='song_data'):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
